*** Première approche : émulation desynchronisée
- On fait une simulation simgrid classique, où les VM s'exécutent en
  exclusion mutuelle, séquentiellement, chacune d'un quantum de temps.
- On veut éviter d'avoir un quantum trop petit (on veut pas pas les
  interrompre à chaque instruction), sinon c'est trop lent.
- Le risque est de recevoir des messages réseau du passé (cad des
  choses qu'on aurait dû prendre en compte à un timestamp simulé
  antérieur), car les réceptions sont passives (=on sait pas à
  l'avance quand on va recevoir)
- Donc on fait des checkpoints, et des rollback en cas de pb
- Pb supplémentaire: l'application est pas forcément déterministe
#+BEGIN_SRC 
def main:
  tant qu'il reste des processus actifs, 
    simgrid_max = simgrid.get_next_receive_time() # Date d'arrivée du prochain message déjà en vol (ou MAX_INT si pas de msg
    achieved_time = quantum(simgrid_max)

def quantum(target_time):
  for vm in vms:
    time = vm.try_go(target_time)
    if time < target_time: # la VM a pas pu aller à target sans faire chier qqn
      rollback les autres, sauf vm (celui qui fait chier)
      return quantum(time)
  # Toutes les VMS ont réussi à avancer jusqu'à target_time
  checkpoint_all()

def VM.trygo(target_time): # tente d'avancer jusqu'à la date indiquée
  simulation en temps réelle, en arrêtant dès que:
    - le temps arrive à target_time 
      dans ce cas, on return target_time
    - on envoie un message tel que m.fin < msg.recv.curr_time
      dans ce cas on return vm.curr_time
#+END_SRC
- ça devrait marcher comme ça (l'algo semble complet et fonctionnel)
  mais ça fait des checkpoints à tout va, ça risque d'être trop lent
  en pratique pour être utile
*** Approche possible: émulation synchronisée
- On exécute les VM de façon concurrente, en s'assurant que leurs
  horologes restent suffisament synchronisées pour qu'on puisse pas
  recevoir de messages du passé.
- Pour l'implémenter dans les VM:
  - SimGrid exporte une date butoir dans une mémoire partagée
  - quand les VM ont envie d'incrémenter leur TSC (=horloge
    matérielle), elles font une attente active jusqu'à ce que la date
    butoir de la mémoire partagée soit suffisament avancée pour
    qu'elles aient le droit d'avancer.
  - Les mises à jour par SimGrid de la date butoir se font avec des
    opérations atomic sur le processeur.
  - C'est possible seulement si la TSC est émulée, pas sur de la
    virtualisation rapide gérée par le matériel
  - Ca semble faisable en pratique dans le code des VM
  - Le DVFS peut géner ce mécanisme:
    - la TSC c'est le compteur de cycle, et c'est traditionnellement
      utilisé pour mesurer le temps qui passe. Pb: si le processeur va
      plus ou moins vite, il a l'impression que le temps va plus ou
      moins vite. Pour corriger ça, quand le CPU modifie la valeur de
      TSC, l'incrément utilisé dépend de la fréquence courante.
    - Si on veut tenir compte de ça, faut synchroniser TSCs et horloge
      centrale dans les deux sens (s'assurer que l'horloge centrale ne
      double pas les VM)
    - Mais c'est quand même plus simple d'interdire le DVFS dans tout ça
- Pour décider la valeur de la date butoir:
  - min(current + latency, prochaine_reception)
- Interface
  - A->VM
    - init_time(t)
    - deliver_now(data)
    - go_to_deadline(t)
  - VM->A
    - i_am_at_deadline()
    - sending(t, data)

#+BEGIN_SRC 
double deadline=0
barrier fin_slot(nb VM + 1) # pour simgrid
B: vecteur d'événements en attente # TODO: c'est moins pénible en concurrent de le couper en un vecteur par VM
comms: vecteur de réceptions en cours

Boucle principale de l'acteur SimGrid coordonant les VMs
  time = 0
  tant qu’il reste des VM actives,
    next_reception_time = network_model.next_event()
    deadline = min (time + min_latency, next_reception_time)

    barrier(fin_slot) # les VM ont avancé jusqu'à deadline

    trier B par temps d’émission croissant
    pour chaque (m_e, t_e) dans B
      this_actor::sleep_until(t_e)
      MB.put_async( m_e )->detach()
      comms.push_back( MB.get_async() )
    B = {}
    this_actor::sleep_until(deadline). Invariant: on n'a aucune réception entre time et deadline

    # on envoie les eventuels messages sortant de SimGrid dans les vm de destination
    comm = test_any(comms)
    while( comm != nullptr ) { # deadline was on next_reception_time, ie, latency was high enough for the next msg to arrive before this
      comm.dest_vm.deliver_now( comm.data )
      comm = test_any(comms)
    }

Actor::barrier():
  parked_vm = 0;
  while (parked_vm < num_vm) {
    fd = select();
    msg = recv(fd);
    switch (msg.type) {
      case DEADLINE:
        parked_vm++;
        break;
      case SEND:
        B.push_back(VM, msg.data, msg.t);
        break;
    }
  }

VM::increment_de_TSC()
  if temps(TSC) <= deadline:
    update_TSC()
  barrier(fin_slot)
  while temps(TSC) >= deadline:
    pass

VM::barrier()
  i_am_at_deadline()

VM::emission(msg)
  sending(current_time, msg)
  
#+END_SRC
- Implémentations
  - Sur Qemu
  - Sur processus : fake-vm
    - Fonctions d'observation du temps wrappées par une lib
      - vsg_gettimeofday()
      - vsg_rdtsc()
    - Communications wrappées par la lib
      - vsg_send()
      - vsg_recv_cb() (appelé en asynchrone sur réception d'un paquet)
    - proxy entre l'acteur SimGrid et l'application
      - maintient le décalage entre l'heure réelle (horloge, TSC) et l'heure
        simulée
      - suspend (resp. réveille) l'application avec SIGSTOP (resp. SIGCONT)
      - fait le relais entre l'acteur et l'application pour les transmission
        de paquets
#+BEGIN_SRC
global next_deadline
global time_offset
global recv_packet_time
global in_recv_packet = false

vsg_getttimeofday() {
      if (in_recv_packet)
            return recv_packet_time
      return gettimeofday() - time_offset
}

vsg_rdtsc() {
      if (in_recv_packet)
            return recv_packet_time
      return rdtsc() - time_offset
}

FakeVM::init() {
      // Initialiser mémoire réception de messages
      // Initialiser timer pour gestion des deadlines, handler = deadline_handler()
      deadline_handler()
}

// Appelé en contexte de traitement de signal
FakeVM::deadline_handler() {
      local start, stop

      start = gettimeofday()

      recv_packet_time = next_deadline
      in_recv_packet = true

      do {
            actor_msg = recv(actor_socket)
            switch (actor_msg.type) {
            case GoToDeadline:
                  break;
            case DeliverPacket:
                  vsg_recv_cb(actor_msg.packet)
                  break;
            }
      } while (actor_msg.type != GoToDeadline);

      in_recv_packet = false

      next_deadline = actor_msg.deadline
      setitimer(next_deadline)

      stop = gettimeofday()
      time_offset += stop - start
}
#+END_SRC
    - contraintes sur les applications fake-vm:
      - passer par l'API vsg pour observation du temps et communications
        réseau
      - mono-thread
      - ne pas masquer le signal dédié à la gestion du temps
      - recv_cb() doit:
        - être appelable en handler de signal avec tous les autres signaux
          masqués (seul contexte dans lequel vsg appelle la fonction)
        - supporter le fait que le temps est figé pour la durée de son
          exécution et pour plusieurs exécutions successives (cas particulier
          de réception de plusieurs messages en même temps)
- Glue entre coordinateur sur SimGrid (VmsInterface) et proxy de la VM
  - noms VMs = @IPv4 en string (16 bytes)
  - VmsInterface utilise un fichier décrivant les VMs à lancer et leurs paramètres
    - crée socket UNIX serveur
    - fork+exec les VMs avec en paramètre obligatoire l'heure initiale
      - dégager VSG_INIT_TIME des messages à échanger
      - éventuellement nom de la VM en paramètre (si programme pas
      spécifique à une @IP)
  - les VMs se connectent à la socket de VmsInterface une fois initialisation
    terminée et prête à s'exécuter
    - = synchro avec VmsInterface
  - contenu d'un packet reçu depuis une VM =
    - nom destinataire (string sur 16 octets dont NUL final)
    - suivi de payload de vsg_packet.size - 16 octets
  - contenu d'un packet envoyé depuis l'acteur à une VM =
    - nom source (string sur 16 octets dont NUL final)
    - suivi de payload de vsg_packet.size - 16 octets
  - notion VM active = socket UNIX avec VmsInterface pas fermée
    - critère d'arrêt d'une VM à spécifier via ses paramètres
      (indépendamment de VmsInterface)
    - dans VmsInterface, choix à l'initialisation entre
      - arrêt de l'ensemble des VMs dès que une s'arrête
      - arrêt non synchronisé des VMs
  - si simulation continue avec des VMs inactives
    - VmsInterface jette les messages au moment où il devrait appeler
      deliverMesssage
- Présupposés sur le workload
  - Pas d'impact sur des machines distantes sans payer le temps de latence
    - En particulier, pas de cancel de communication simgrid
  - Ca devrait mal se passer avec notre façon d'avancer NS3 (pas idempotent)

*** Cas de test
- micro-benchmark
      - différentes fonctions d'observation du temps par
        rapport aux communications réseau
            - dans le cas fake-vm, un mode d'exécution dit "réel" rend les
              implémentations wrappées identique aux implémentations natives
                    - vsg_send() et vsg_recv_cb() communiquent directement
                      avec les autres éléments de l'application
            - horloge
                  - RTC
                  - TSC
                        - Test1 avec fake-vm
#+BEGIN_SRC
Application:
      int loop_count;
      int quantity = NUM_TSC_TICKS;
      start = my_rdtsc();
      for (loop_count = 0; loop_count < MAX_INT; loop_count++) {
            if (rdtsc() - start > NUM_TSC_TICKS)
                  break;
      }
      print(loop_count);

Vérifier que loop_count ne varie pas trop entre exécution sur VSG et exécution réelle
#+END_SRC

                  - autres timers
            - comparaisons avec une horloge distante
            - quantité de travail pouvant être faite le
              temps d'un aller/retour réseau ou entre deux
              réceptions à intervalle de temps prédictible
                  - nombre d'itérations d'une boucle
                  - ...
- à partir d'un malware
      - reprendre comportement réseau d'un malware récent
            - Wannacry, ...
      - écrire un programme simulant le comportement
            - y inclure des fonctions d'observation du temps
- dans tous les cas, comparer observations du temps du programme en exécution sur
  un réseau réel avec observations en exécution sur VM+SimGrid
  (vsg)

*** TODO:
- Chercher un moyen de faire des POC pour comparer les approches
- Se documenter sur le comportement réseau d'un malware pour écrire un
  modèle
- Réfléchir sérieusement à comment gérer les communications au niveau
  Ethernet d'une VM
      - Encapsuleur / décapsuleur IP
      - ARP
            - chaque est application est toute seule derrière un routeur IP ?
            - proxy ARP ?
- Explorer l'idée laissée en attente de checkpoints à intervale de
  temps régulier. --> on abandonne
  - La différence avec l'émulation désynchronisée est qu'on tente de
    faire moins de checkpoints en laissant plus de un message passer
    entre les checkpoints.
  - Ca demande de savoir checkpointer non seulement les modèles de
    perf SimGrid (LMM) mais aussi la liste des messages en vol (qui
    peuvent être modifiés à la réception).
  - Donc, à priori, c'est plutôt à faire d'un processus séparé que
    depuis l'intérieur du processus SimGrid. Cad, c'est proche de
    l'architecture du MC

*** Idées de stage potentiel

  - Time-accurate Network Simulation Interconnecting QEMU VMs (TANSIV)


  - Bridging Virtual Machines to Simulated 
  - Faire une version modifiée de QEMU qui se branche sur le proto existant
    - Travail surtout focus sur l'environnement réseau:
      - V1: modif SLIRP qui balance sur SG, au lieu de sortir sur un
        vrai réseau.C'est pas ultime car le réseau est forcément
        NATé, mais c'est un bon début. Peut-être qu'on aura déjà fait
        ça au début du stage. Peut-être.
      - V2: Tjs une modif de SLIRP, mais sans l'aspect NAT.
    - Le proto "existant" doit contenir la transmission des comms à
      travers simgrid. Pour l'instant il manque
      - L'ouverture des entêtes pour trouver l'IP dest
      - Interception du temps indétectable, qui est indispensable à
        l'ensemble mais qui ferait trop pour l'étudiant.
    - Avoir un proxy ARP (répond tjs "c'est moi" à chaque requete)
      - Y'a qqch dont on peut s'inspirer dans la pile slurp
    - => c'est là qu'est le gros du travail du stage
    - Fallback possible sur une solution docker+TAP
  - Refaire un environnement réseau: peut-être avec EnosLib?
    - Générer les configs, les disques virtuels, les /etc de chacun
    - DNS:
      - on peut faire un (ou +ieurs) vrais serveurs dans des VM
      - Y'a un DNS virtuel dans la pile slirp (et un SMB)
    - => c'est pas drôle, mais pas dur
  - Faire des expérimentations avec du vrai code
    - Code "normal", pas malicieux
      - Une infra sharelatex un peu simple
      - les tests d'intégration Ceph si possible, mais semble ambitieux
* Journal
** 23 janvier
- Mt n'a pas avancé sur le sujet de stage, même pas commité les notes
  de la dernière fois...
- Benjamin:
  - Fini sur le fichier de déploiement. On passe assez d'infos pour
    fork+exec les VM 
  - On crée 1 acteur à chaque envoi applicatif, et y'a un permanent
    receiver par mailbox qui peut potentiellement recevoir. C'est
    beaucoup mais les solutions plus économes en ressource simgrid
    sont compliquées à faire marcher dans tous les cas et SG devrait
    tenir la charge.
- Que faire quand on parle à qqun de déjà mort/pas encore né
  - S'il est déjà mort, on laisse les paquets transiter dans SimGrid
    jusqu'à destination, et l'interface vers les VM ignore les données
  - S'il n'est pas encore né, le coordinateur ne sait pas router,
    alors il ignore les données
- Gestion du temps
  - On ajoute dans chaque VM un offset pour convertir entre d'une part
    l'epoch donné par simgrid et d'autre part l'heure locale de la
    VM. Ca peut permettre de gérer des VM qui ne sont pas dans la même
    timezone.
